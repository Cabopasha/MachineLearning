â€¢	Makine Ã–ÄŸrenmesi (Machine Learning): Verilerden Ã¶rÃ¼ntÃ¼ler veya kurallar Ã¶ÄŸrenerek tahmin veya karar verme yapabilen algoritmalar bÃ¼tÃ¼nÃ¼dÃ¼r.
â€¢	Matris ManipÃ¼lasyonu: Matrisler Ã¼zerinde yapÄ±lan iÅŸlemlerdir (toplama, Ã§arpma, transpoz alma, tersini alma gibi). Makine Ã¶ÄŸrenmesinde veri yapÄ±larÄ± ve model hesaplamalarÄ± genellikle matrisler aracÄ±lÄ±ÄŸÄ±yla ifade edilir.
â€¢	Ã–zdeÄŸer (Eigenvalue) ve Ã–zvektÃ¶r (Eigenvector):
o	Bir matris A iÃ§in, Av=Î»v =eÅŸitliÄŸini saÄŸlayan Î» skalarÄ±na Ã¶zdeÄŸer, v vektÃ¶rÃ¼ne ise Ã¶zvektÃ¶r denir.
o	Ã–zvektÃ¶r, uygulanan dÃ¶nÃ¼ÅŸÃ¼mde yalnÄ±zca Ã¶lÃ§eklenen ama yÃ¶nÃ¼ deÄŸiÅŸmeyen vektÃ¶rdÃ¼r.
________________________________________
Makine Ã–ÄŸrenmesi ile Ä°liÅŸkisi
1. Matris ManipÃ¼lasyonu
Makine Ã¶ÄŸrenmesi algoritmalarÄ±nÄ±n Ã§oÄŸu, Ã¶zellikle Ã§ok boyutlu veriler (feature vectors) ile Ã§alÄ±ÅŸÄ±r. Bu veriler genellikle matris biÃ§iminde saklanÄ±r ve iÅŸlenir.
â€¢	Ã–rnek:
o	Lojistik regresyonda, tahminler y=Ïƒ(XW+b) ÅŸeklinde matris iÅŸlemleriyle hesaplanÄ±r.
o	Sinir aÄŸlarÄ±nda ileri ve geri yayÄ±lÄ±m (forward/backward propagation) tamamen matris Ã§arpÄ±mlarÄ± ile yapÄ±lÄ±r.
2. Ã–zdeÄŸerler ve Ã–zvektÃ¶rler
Ã–zdeÄŸerler ve Ã¶zvektÃ¶rler, Ã¶zellikle boyut indirgeme ve veri sÄ±kÄ±ÅŸtÄ±rma tekniklerinde Ã¶nemli rol oynar.
â€¢	Principal Component Analysis (PCA):
o	YÃ¼ksek boyutlu verilerde en fazla varyansa sahip yÃ¶nleri bulmak iÃ§in kullanÄ±lÄ±r.
o	AdÄ±mlar:
1.	Veriyi normalize et.
2.	Kovaryans matrisini hesapla.
3.	Kovaryans matrisinin Ã¶zdeÄŸerlerini ve Ã¶zvektÃ¶rlerini bul.
4.	En bÃ¼yÃ¼k Ã¶zdeÄŸere sahip Ã¶zvektÃ¶rler, en anlamlÄ± yÃ¶nlerdir (principal components).
5.	Veri bu yeni uzaya yansÄ±tÄ±lÄ±r â†’ boyut indirgenmiÅŸ veri elde edilir.
â€¢	Spektral KÃ¼meleme (Spectral Clustering):
o	Ã–zellikle doÄŸrusal olmayan kÃ¼meleme problemlerinde kullanÄ±lÄ±r.
o	AdÄ±mlar:
ï‚§	Benzerlik grafÄ± oluÅŸturulur.
ï‚§	Laplasyen matrisi oluÅŸturulur.
ï‚§	Laplasyenin Ã¶zvektÃ¶rleri alÄ±nÄ±r â†’ bu vektÃ¶rler yeni bir uzayda veri temsili saÄŸlar.
â€¢	Yapay Sinir AÄŸlarÄ±nÄ±n AÄŸÄ±rlÄ±k Analizi:
o	EÄŸitim sÄ±rasÄ±nda aÄŸÄ±rlÄ±k matrislerinin Ã¶zdeÄŸer daÄŸÄ±lÄ±mÄ±, aÄŸÄ±n Ã¶ÄŸrenme kapasitesini ve genelleme yeteneÄŸini analiz etmekte kullanÄ±lÄ±r.
________________________________________
KullanÄ±ldÄ±ÄŸÄ± YÃ¶ntemler ve YaklaÅŸÄ±mlar
YÃ¶ntem/YaklaÅŸÄ±m	KullanÄ±m AlanÄ±	Ã–zdeÄŸer/Ã–zvektÃ¶r Ä°liÅŸkisi
PCA	Boyut indirgeme, gÃ¶rselleÅŸtirme	Kovaryans matrisinin Ã¶zdeÄŸer analizi
LDA (Linear Discriminant Analysis)	SÄ±nÄ±flandÄ±rma, boyut indirgeme	Scatter matrislerinin Ã¶zdeÄŸerleri
Spektral KÃ¼meleme	KÃ¼meleme (clustering)	Laplasyen matrisinin Ã¶zvektÃ¶rleri
Kernel PCA	DoÄŸrusal olmayan boyut indirgeme	Kernel matrislerinin Ã¶zdeÄŸerleri
Singular Value Decomposition (SVD)	Veri sÄ±kÄ±ÅŸtÄ±rma, Ã¶neri sistemleri	Ã–zdeÄŸer analiziyle iliÅŸkili
______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

numpy.linalg.eig Fonksiyonu
 Fonksiyonun AmacÄ±
numpy.linalg.eig fonksiyonu, bir kare matrisin Ã¶zdeÄŸerlerini ve saÄŸ Ã¶zvektÃ¶rlerini hesaplar. Yani, verilen bir kare matris A iÃ§in, aÅŸaÄŸÄ±daki denklemi saÄŸlayan Î»(Ã¶zdeÄŸer) ve v(Ã¶zvektÃ¶r) Ã§iftlerini bulur:
 					Av=Î»vA 
 SÃ¶z Dizimi
numpy.linalg.eig(a)
Parametreler:
â€¢	a: Kare bir matris (boyutlarÄ± M x M )
DÃ¶nÃ¼ÅŸ DeÄŸeri:
â€¢	w: Matrisin Ã¶zdeÄŸerlerini iÃ§eren bir dizi
â€¢	v: Matrisin saÄŸ Ã¶zvektÃ¶rlerini iÃ§eren bir dizi; her sÃ¼tun, w[i] Ã¶zdeÄŸerine karÅŸÄ±lÄ±k gelen Ã¶zvektÃ¶rdÃ¼r
Notlar:
â€¢	Elde edilen Ã¶zdeÄŸerler, genellikle karmaÅŸÄ±k sayÄ±lar olabilir.
â€¢	Ã–zvektÃ¶rler, birim uzunlukta (normalize edilmiÅŸ) olacak ÅŸekilde dÃ¶ndÃ¼rÃ¼lÃ¼r.
ğŸ§ª Ã–rnek KullanÄ±m
import numpy as np

A = np.array([[1, -2],
            	         [1,  3]])

eigenvalues, eigenvectors = np.linalg.eig(A)

print("Ã–zdeÄŸerler:", eigenvalues)
print("Ã–zvektÃ¶rler:\n", eigenvectors)
Ã‡Ä±ktÄ±:
Ã–zdeÄŸerler: [2.+1.j 2.-1.j]
Ã–zvektÃ¶rler:
 [[ 0.81649658+0.j          0.81649658-0.j        ]
 [-0.40824829-0.40824829j -0.40824829+0.40824829j]]
Bu Ã¶rnekte, matrisin karmaÅŸÄ±k Ã¶zdeÄŸerleri ve bunlara karÅŸÄ±lÄ±k gelen Ã¶zvektÃ¶rleri hesaplanmÄ±ÅŸtÄ±r.
________________________________________
Kaynak Kod Ä°ncelemesi
numpy.linalg.eig fonksiyonu, NumPy'nin linalg modÃ¼lÃ¼nde tanÄ±mlanmÄ±ÅŸtÄ±r.
 Ã–nemli Dosyalar
â€¢	linalg.py: YÃ¼ksek seviyeli lineer cebir fonksiyonlarÄ±nÄ±n tanÄ±mlandÄ±ÄŸÄ± dosya.
â€¢	lapack_lite: LAPACK (Linear Algebra PACKage) kÃ¼tÃ¼phanesinin hafif bir versiyonu; dÃ¼ÅŸÃ¼k seviyeli lineer cebir iÅŸlemleri iÃ§in kullanÄ±lÄ±r.
â€¢	umath_linalg.cpp: C++ ile yazÄ±lmÄ±ÅŸ olup, performans kritik lineer cebir iÅŸlemlerini iÃ§erir.
eig fonksiyonu, genellikle LAPACK kÃ¼tÃ¼phanesindeki dgeev (gerÃ§ek matrisler iÃ§in) veya zgeev (karmaÅŸÄ±k matrisler iÃ§in) rutinlerini Ã§aÄŸÄ±rarak Ã¶zdeÄŸer ve Ã¶zvektÃ¶r hesaplamalarÄ±nÄ± gerÃ§ekleÅŸtirir. Bu sayede, yÃ¼ksek performanslÄ± ve gÃ¼venilir sonuÃ§lar elde edilir.
________________________________________
 Ek Bilgiler
â€¢	numpy.linalg.eig fonksiyonu, genellikle PCA (Principal Component Analysis), LDA (Linear Discriminant Analysis) gibi boyut indirgeme tekniklerinde kullanÄ±lÄ±r.
â€¢	EÄŸer matrisiniz simetrik veya Hermitian ise, numpy.linalg.eigh fonksiyonunu kullanmanÄ±z daha uygundur; bu fonksiyon, simetrik matrisler iÃ§in optimize edilmiÅŸtir.
Kaynaklar: https://www.veribilimiokulu.com/makine-ogrenmesine-cok-degiskenli-istatistiksel-yaklasimlar-temel-bilesenler-analizi/2/
https://tr.d2l.ai/chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html
https://chatgpt.com/
https://gemini.google.com/



